{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Siniestros Viales en Buenos Aires (2019-2023)\n",
    "\n",
    "Este notebook documenta el proceso de carga, limpieza y preparación de datos de siniestros viales en la Ciudad de Buenos Aires para análisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos y configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente:\n",
      "- Hechos: 44012 registros, 23 columnas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carga de archivos desde la fuente original\n",
    "# - siniestros_viales_hechos.xlsx: Información sobre los eventos/accidentes\n",
    "\n",
    "try:\n",
    "    # Usando rutas relativas para mejor portabilidad\n",
    "    df_accidents = pd.read_excel(\"../data/raw/siniestros_viales_hechos.xlsx\")\n",
    "    \n",
    "    print(f\"Datos cargados exitosamente:\")\n",
    "    print(f\"- Hechos: {df_accidents.shape[0]} registros, {df_accidents.shape[1]} columnas\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error al cargar archivos: {e}\")\n",
    "    print(\"Verifique que los archivos se encuentren en la ruta correcta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración inicial y diagnóstico de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de valores nulos por columna en el dataset de accidentes:\n",
      "id_hecho                     0\n",
      "n_victimas                   0\n",
      "fecha                        0\n",
      "aaaa                         0\n",
      "mm                           0\n",
      "dd                           0\n",
      "hora                         0\n",
      "hh                           0\n",
      "direccion_normalizada        0\n",
      "calle                    10901\n",
      "altura                   11347\n",
      "cruce                    14503\n",
      "otra_direccion              31\n",
      "comuna                       0\n",
      "tipo_de_calle                0\n",
      "geocodificacion_caba         0\n",
      "longitud                   216\n",
      "latitud                    216\n",
      "participantes                0\n",
      "victima                      0\n",
      "contraparte                  0\n",
      "gravedad                     0\n",
      "tipo_de_dato                 0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos en el dataset de accidentes:\n",
      "id_hecho                         object\n",
      "n_victimas                        int64\n",
      "fecha                    datetime64[ns]\n",
      "aaaa                              int64\n",
      "mm                                int64\n",
      "dd                                int64\n",
      "hora                             object\n",
      "hh                               object\n",
      "direccion_normalizada            object\n",
      "calle                            object\n",
      "altura                           object\n",
      "cruce                            object\n",
      "otra_direccion                   object\n",
      "comuna                           object\n",
      "tipo_de_calle                    object\n",
      "geocodificacion_caba             object\n",
      "longitud                         object\n",
      "latitud                          object\n",
      "participantes                    object\n",
      "victima                          object\n",
      "contraparte                      object\n",
      "gravedad                         object\n",
      "tipo_de_dato                     object\n",
      "dtype: object\n",
      "\n",
      "Resumen estadístico del dataset de accidentes:\n",
      "         n_victimas                          fecha          aaaa  \\\n",
      "count  44012.000000                          44012  44012.000000   \n",
      "mean       1.153185  2021-08-08 13:22:54.497864192   2021.091952   \n",
      "min        1.000000            2019-01-01 00:00:00   2019.000000   \n",
      "25%        1.000000            2020-02-07 00:00:00   2020.000000   \n",
      "50%        1.000000            2021-10-14 00:00:00   2021.000000   \n",
      "75%        1.000000            2022-12-08 00:00:00   2022.000000   \n",
      "max       18.000000            2023-12-31 00:00:00   2023.000000   \n",
      "std        0.565237                            NaN      1.489238   \n",
      "\n",
      "                 mm            dd  \n",
      "count  44012.000000  44012.000000  \n",
      "mean       6.672067     15.486367  \n",
      "min        1.000000      1.000000  \n",
      "25%        4.000000      8.000000  \n",
      "50%        7.000000     15.000000  \n",
      "75%       10.000000     23.000000  \n",
      "max       12.000000     31.000000  \n",
      "std        3.422118      8.752242  \n"
     ]
    }
   ],
   "source": [
    "# Verificar valores nulos en el dataset\n",
    "print(\"Cantidad de valores nulos por columna en el dataset de accidentes:\")\n",
    "print(df_accidents.isnull().sum())\n",
    "\n",
    "\n",
    "# Examinar tipos de datos\n",
    "print(\"\\nTipos de datos en el dataset de accidentes:\")\n",
    "print(df_accidents.dtypes)\n",
    "\n",
    "# Resumen estadístico básico para variables numéricas\n",
    "print(\"\\nResumen estadístico del dataset de accidentes:\")\n",
    "print(df_accidents.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploración de valores únicos en dataset de accidentes:\n",
      "\n",
      "Valores únicos en id_hecho (44012 valores):\n",
      "['LC-2019-0008283' 'LC-2019-0007634' 'LC-2019-0008974' 'LC-2019-0010983'\n",
      " 'LC-2019-0011092' 'LC-2019-0017186' 'LC-2019-0022256' 'LC-2019-0687058'\n",
      " 'LC-2019-0688515' 'LC-2019-0757409'] ... y 44002 más\n",
      "\n",
      "Valores únicos en n_victimas (17 valores):\n",
      "[ 2  1  4  3  5 13  6  8  9  7] ... y 7 más\n",
      "\n",
      "Valores únicos en fecha (1826 valores):\n",
      "<DatetimeArray>\n",
      "['2019-01-04 00:00:00', '2019-01-05 00:00:00', '2019-01-06 00:00:00',\n",
      " '2019-01-07 00:00:00', '2019-01-09 00:00:00', '2019-01-11 00:00:00',\n",
      " '2019-01-12 00:00:00', '2019-01-13 00:00:00', '2019-01-15 00:00:00',\n",
      " '2019-01-17 00:00:00']\n",
      "Length: 10, dtype: datetime64[ns] ... y 1816 más\n",
      "\n",
      "Valores únicos en aaaa (5 valores):\n",
      "[2019 2020 2021 2022 2023]\n",
      "\n",
      "Valores únicos en mm (12 valores):\n",
      "[ 1  2  3  4  5  6  7  8  9 10] ... y 2 más\n",
      "\n",
      "Valores únicos en dd (31 valores):\n",
      "[ 4  5  6  7  9 11 12 13 15 17] ... y 21 más\n",
      "\n",
      "Valores únicos en hora (2607 valores):\n",
      "[datetime.time(17, 53) datetime.time(14, 50) datetime.time(9, 30)\n",
      " datetime.time(22, 20) datetime.time(3, 50) datetime.time(11, 55)\n",
      " datetime.time(12, 10) datetime.time(9, 20) datetime.time(16, 58)\n",
      " datetime.time(6, 45)] ... y 2597 más\n",
      "\n",
      "Valores únicos en hh (26 valores):\n",
      "[17 14 9 22 3 11 12 16 6 2] ... y 16 más\n",
      "\n",
      "Valores únicos en direccion_normalizada (17538 valores):\n",
      "['SD' 'PAZ, GRAL. AV. y ACHA, MARIANO, GRAL.' 'DEL LIBERTADOR AV. 100'\n",
      " 'TRONADOR y GIRIBONE' 'PASEO COLON AV. y MEXICO'\n",
      " 'LACROZE, FEDERICO y CAMPOS, LUIS M. AV.' 'QUESADA y MILLER'\n",
      " 'CASTILLO y CARRANZA, ANGEL JUSTINIANO' 'DIAZ VELEZ AV. y PRINGLES'\n",
      " 'MU?IZ 568'] ... y 17528 más\n",
      "\n",
      "Valores únicos en calle (1249 valores):\n",
      "[nan 'PAZ, GRAL. AV.' 'DEL LIBERTADOR AV.' 'TRONADOR' 'PASEO COLON AV.'\n",
      " 'LACROZE, FEDERICO' 'QUESADA' 'CASTILLO' 'DIAZ VELEZ AV.' 'MU?IZ'] ... y 1239 más\n",
      "\n",
      "Valores únicos en altura (3184 valores):\n",
      "[nan 100 568 3400 211 4006 1941 4937 4851 5870] ... y 3174 más\n",
      "\n",
      "Valores únicos en cruce (1423 valores):\n",
      "[nan 'ACHA, MARIANO, GRAL.' 'GIRIBONE' 'MEXICO' 'CAMPOS, LUIS M. AV.'\n",
      " 'MILLER' 'CARRANZA, ANGEL JUSTINIANO' 'PRINGLES'\n",
      " 'BALBIN, RICARDO, DR. AV.' 'DIAZ, CESAR, GRAL.'] ... y 1413 más\n",
      "\n",
      "Valores únicos en otra_direccion (17209 valores):\n",
      "['aut. Dellepiane altura cruce con la calle Timoteo Gordillo , Barrio: No Especificada  , jurisdicción comisaria:'\n",
      " 'SD'\n",
      " 'AV. CRUZ ENTRE AV. ESCALADA Y AUTOPISTA CÁMPORA , Barrio: No Especificada  , jurisdicción comisaria:'\n",
      " 'gral conesa , Barrio:   , jurisdicción comisaria:'\n",
      " 'Corrientes Av. 3358, Barrio:   , jurisdicción comisaria:'\n",
      " 'AUTOPISTA 25 DE MAYO, ALTURA KM 1.5 (CARABOBO) , Barrio: No Especificada  , jurisdicción comisaria:'\n",
      " 'Avenida Perito Moreno y la Fuente , Barrio: No Especificada  , jurisdicción comisaria:'\n",
      " 'AUTOPISTA ARTURO ILLIA 2,2 KM, Barrio: No Especificada  , jurisdicción comisaria:'\n",
      " '9 De Julio Av. y Belgrano Av. 0, Barrio:   , jurisdicción comisaria:'\n",
      " 'Avenida Monseñor Bufano 5975, Barrio:   , jurisdicción comisaria:'] ... y 17199 más\n",
      "\n",
      "Valores únicos en comuna (17 valores):\n",
      "['SD' 'sd' 9 8 7 6 5 4 3 2] ... y 7 más\n",
      "\n",
      "Valores únicos en tipo_de_calle (8 valores):\n",
      "['SD' 'AUTOPISTA' 'CALLE' 'AVENIDA' 'calle' 'sd' 'AV. GRAL. PAZ'\n",
      " 'Autopista']\n",
      "\n",
      "Valores únicos en geocodificacion_caba (26795 valores):\n",
      "['SD' 'Point (96982.6358422506 108957.813038542)'\n",
      " 'Point (108067.125594 104023.680363)'\n",
      " 'Point (99744.1439377455 105520.728400727)'\n",
      " 'Point (108629.964506752 101606.761130562)'\n",
      " 'Point (102108.128777288 107087.128651998)'\n",
      " 'Point (97962.881936702 107021.636764033)'\n",
      " 'Point (101826.896937008 104708.368665252)'\n",
      " 'Point (103261.483699331 102281.12003418)'\n",
      " 'Point (103336.312114 101018.339551)'] ... y 26785 más\n",
      "\n",
      "Valores únicos en longitud (23984 valores):\n",
      "['SD' nan '-58.405475' '-58.485836' '-58.406528' '-58.379840' '-58.514578'\n",
      " '-58.377772' '-58.456939' '-58.388314'] ... y 23974 más\n",
      "\n",
      "Valores únicos en latitud (23226 valores):\n",
      "['SD' nan '-34.604562' '-34.687677' '-34.646861' '-34.627851' '-34.640543'\n",
      " '-34.587895' '-34.565036' '-34.626201'] ... y 23216 más\n",
      "\n",
      "Valores únicos en participantes (199 valores):\n",
      "['AUTO-SD' 'MOTO-SD' 'BICICLETA-SD' 'PEATON-SD' 'SD-SD' 'UTILITARIO-SD'\n",
      " 'MONOPATIN-SD' 'TRANSPORTE PUBLICO-SD' 'CAMION-SD' 'OTRO-SD'] ... y 189 más\n",
      "\n",
      "Valores únicos en victima (21 valores):\n",
      "['AUTO' 'MOTO' 'BICICLETA' 'PEATON' 'SD' 'UTILITARIO' 'MONOPATIN'\n",
      " 'TRANSPORTE PUBLICO' 'CAMION' 'OTRO'] ... y 11 más\n",
      "\n",
      "Valores únicos en contraparte (23 valores):\n",
      "['SD' 'AUTO' 'UTILITARIO' 'TRANSPORTE PUBLICO' 'OBJETO FIJO' 'TAXI'\n",
      " 'PEATON' 'MOTO' 'BICICLETA' 'CAMION'] ... y 13 más\n",
      "\n",
      "Valores únicos en gravedad (5 valores):\n",
      "['SD' 'GRAVE' 'sd' 'grave' 'MORTAL']\n",
      "\n",
      "Valores únicos en tipo_de_dato (2 valores):\n",
      "['definitivo' 'provisorio']\n"
     ]
    }
   ],
   "source": [
    "# Función para mostrar valores únicos limitando la cantidad para mejor visualización\n",
    "def show_unique_values(df, max_items=10):\n",
    "    \"\"\"\n",
    "    Muestra los valores únicos de cada columna en un DataFrame\n",
    "    \n",
    "    Parámetros:\n",
    "    df (pandas.DataFrame): DataFrame a explorar\n",
    "    max_items (int): Número máximo de valores únicos a mostrar por columna\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"\\nValores únicos en {column} ({len(unique_values)} valores):\")\n",
    "        \n",
    "        # Si hay muchos valores únicos, mostrar solo algunos\n",
    "        if len(unique_values) > max_items:\n",
    "            print(unique_values[:max_items], '... y', len(unique_values) - max_items, 'más')\n",
    "        else:\n",
    "            print(unique_values)\n",
    "\n",
    "# Explorar valores únicos en cada dataset\n",
    "print(\"Exploración de valores únicos en dataset de accidentes:\")\n",
    "show_unique_values(df_accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limpieza y transformación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Eliminación de columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas: ['direccion_normalizada', 'cruce', 'otra_direccion', 'comuna', 'tipo_de_calle']\n",
      "Nuevas dimensiones del dataset: (44012, 18)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos columnas redundantes o que no aportan valor para el análisis\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"direccion_normalizada\", # Redundante\n",
    "    \"cruce\",                 # No es necesario para identificar la dirección\n",
    "    \"otra_direccion\",        # Información redundante\n",
    "    \"comuna\",                # Se puede obtener por geocodificación posteriormente si es necesario\n",
    "    \"tipo_de_calle\"          # No contiene suficiente información para el análisis\n",
    "]\n",
    "\n",
    "df_accidents.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"Columnas eliminadas: {columns_to_drop}\")\n",
    "print(f\"Nuevas dimensiones del dataset: {df_accidents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalización de direcciones\n",
    "\n",
    "En esta sección, implementamos funciones para normalizar y completar las direcciones y las coordenadas a partir de diferentes fuentes de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44012/44012 [2:54:58<00:00,  4.19it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías para geocodificación\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Activar barra de progreso para pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "def is_valid(value):\n",
    "    \"\"\"\n",
    "    Verifica si un valor es válido (no es nulo ni valor de relleno)\n",
    "    \n",
    "    Parámetros:\n",
    "    value: Valor a verificar\n",
    "    \n",
    "    Retorna:\n",
    "    bool: True si el valor es válido, False en caso contrario\n",
    "    \"\"\"\n",
    "    return not pd.isna(value) and value not in {\"\", \"sd\", \"SD\", \"0\", 0}\n",
    "\n",
    "def obtener_calle_altura_from_geocodificacion(geocodificacion, max_retries=10):\n",
    "    \"\"\"\n",
    "    Obtiene la calle y altura a partir de las coordenadas en formato POINT\n",
    "    usando la API del USIG (Buenos Aires)\n",
    "    \n",
    "    Parámetros:\n",
    "    geocodificacion (str): Coordenadas en formato \"POINT(x y)\"\n",
    "    max_retries (int): Número máximo de reintentos en caso de fallo\n",
    "    \n",
    "    Retorna:\n",
    "    str: Dirección normalizada o None si no se pudo obtener\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            if not isinstance(geocodificacion, str) or not geocodificacion.lower().startswith(\"point\"):\n",
    "                return None\n",
    "                \n",
    "            # Extraer coordenadas del formato POINT\n",
    "            values = geocodificacion.lower().replace(\"point(\", \"\").replace(\"point (\", \"\").replace(\")\", \"\").replace(\",\", \" \").split()\n",
    "            if len(values) < 2:\n",
    "                return None\n",
    "                \n",
    "            x, y = values[0], values[1]\n",
    "            url = f\"http://ws.usig.buenosaires.gob.ar/geocoder/2.2/reversegeocoding?x={x}&y={y}\"\n",
    "            \n",
    "            # Realizar solicitud a la API\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.text[2:-2] == \"ErrorCoordenadasErroneas\":\n",
    "                return \"ErrorCoordenadasErroneas\"\n",
    "                \n",
    "            if response.status_code == 200:\n",
    "                txt = response.text[1:-1]\n",
    "                data = json.loads(txt)\n",
    "                return data[\"puerta\"]\n",
    "                \n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Esperar antes de reintentar\n",
    "        except Exception as e:\n",
    "            print(f\"Error en geocodificación: {e}\")\n",
    "            return None\n",
    "            \n",
    "    return None  # Si se agotan los reintentos\n",
    "\n",
    "def obtener_calle_altura_from_coordinates(lat, lon, max_retries=5):\n",
    "    \"\"\"\n",
    "    Obtiene la calle y altura a partir de coordenadas usando OpenStreetMap\n",
    "    \n",
    "    Parámetros:\n",
    "    lat (float): Latitud\n",
    "    lon (float): Longitud\n",
    "    max_retries (int): Número máximo de reintentos en caso de fallo\n",
    "    \n",
    "    Retorna:\n",
    "    str: Dirección normalizada o mensaje de error\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            url = f\"https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json\"\n",
    "            headers = {\"User-Agent\": \"AccidentAnalysis/1.0 (carlosschopfer@gmail.com)\"}  # Es importante identificarse\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            calle = data.get(\"address\", {}).get(\"road\", \"\")\n",
    "            altura = data.get(\"address\", {}).get(\"house_number\", \"\")\n",
    "            \n",
    "            if calle:\n",
    "                return f\"{calle} {altura}\".strip().upper()\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError):\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Esperar antes de reintentar\n",
    "        except Exception as e:\n",
    "            return f\"ERROR: {str(e)}\"\n",
    "            \n",
    "    return None  # Si se agotan los reintentos\n",
    "\n",
    "df_accidents[\"calle_y_altura\"] = None  # Inicializar la columna de dirección\n",
    "\n",
    "# Función principal para obtener calle y altura\n",
    "def obtener_calle_altura(row, backup_counter=1000):\n",
    "    \"\"\"\n",
    "    Obtiene la calle y altura a partir de los datos disponibles en la fila\n",
    "    Intenta diferentes fuentes en orden de prioridad\n",
    "    \n",
    "    Parámetros:\n",
    "    row (Series): Fila del DataFrame\n",
    "    backup_counter (int): Cada cuántas filas se debe realizar un backup\n",
    "    \n",
    "    Retorna:\n",
    "    str: Dirección normalizada o None si no se pudo obtener\n",
    "    \"\"\"\n",
    "    # global i  # Contador para backups periódicos\n",
    "    \n",
    "    # Realizar backup periódico\n",
    "    # if i % backup_counter == 0:\n",
    "    #     df_accidents.to_csv(f\"../data/processed/accidents_backup_{i}.csv\", )\n",
    "        \n",
    "    try:\n",
    "        # Si ya existe, mantenerla\n",
    "        if is_valid(row[\"calle_y_altura\"]):\n",
    "            return row[\"calle_y_altura\"]\n",
    "            \n",
    "        # Si tenemos calle y altura, combinarlas\n",
    "        if is_valid(row[\"calle\"]) and is_valid(row[\"altura\"]):\n",
    "            # i += 1\n",
    "            return f\"{row['calle']} {row['altura']}\"\n",
    "            \n",
    "        # Si tenemos geocodificación, usarla\n",
    "        if is_valid(row[\"geocodificacion_caba\"]):\n",
    "            value = obtener_calle_altura_from_geocodificacion(row[\"geocodificacion_caba\"])\n",
    "            if value != \"ErrorCoordenadasErroneas\" and value is not None:\n",
    "                # i += 1\n",
    "                return value\n",
    "                \n",
    "        # Si tenemos coordenadas, usarlas\n",
    "        if is_valid(row[\"latitud\"]) and is_valid(row[\"longitud\"]):\n",
    "            # i += 1\n",
    "            return obtener_calle_altura_from_coordinates(row[\"latitud\"], row[\"longitud\"])\n",
    "            \n",
    "        return pd.NA\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando fila: {e}\")\n",
    "        return pd.NA\n",
    "\n",
    "# Inicializar contador global\n",
    "# i = 0\n",
    "\n",
    "# Aplicar la función a todo el dataset (puede llevar tiempo)\n",
    "# Descomentar para ejecutar:\n",
    "df_accidents[\"calle_y_altura\"] = df_accidents.progress_apply(obtener_calle_altura, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NA>\n",
      "SANCHEZ DE LORIA 795\n",
      "<NA>\n"
     ]
    }
   ],
   "source": [
    "print(obtener_calle_altura(df_accidents.iloc[0]))  # Probar la función con una fila de ejemplo\n",
    "print(obtener_calle_altura(df_accidents.iloc[117]))  # Probar la función con otra fila de ejemplo\n",
    "print(obtener_calle_altura(df_accidents.iloc[1016]))  # Probar la función con otra fila de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44012/44012 [49:16<00:00, 14.89it/s]  \n"
     ]
    }
   ],
   "source": [
    "def obtener_lat_lon_from_direccion(direccion, max_retries=5):\n",
    "    \"\"\"\n",
    "    Obtiene la latitud y longitud a partir de una dirección usando OpenStreetMap\n",
    "    \n",
    "    Parámetros:\n",
    "    direccion (str): Calle y altura a geocodificar\n",
    "    max_retries (int): Número máximo de reintentos en caso de fallo\n",
    "\n",
    "    Retorna:\n",
    "    tuple: (latitud, longitud) o None si no se pudo obtener\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            url=f\"https://nominatim.openstreetmap.org/search?q={direccion}&format=json&addressdetails=1&limit=1\"\n",
    "            headers = {\"User-Agent\": \"AccidentAnalysis/1.0 (carlosschopfer@gmail.com)\"}  # Es importante identificarse\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            data = response.json()\n",
    "            lat=data[0][\"lat\"]\n",
    "            lon=data[0][\"lon\"]\n",
    "            if lat:\n",
    "                return lat, lon\n",
    "            else:\n",
    "                return pd.NA, pd.NA\n",
    "        except:\n",
    "            retries += 1\n",
    "            time.sleep(2)\n",
    "    return pd.NA, pd.NA  # Si se agotan los reintentos\n",
    "def obtener_lat_lon(row, backup_counter=1000):\n",
    "    \"\"\"\n",
    "    Obtiene la latitud y longitud a partir de los datos disponibles en la fila\n",
    "    Intenta diferentes fuentes en orden de prioridad\n",
    "    \n",
    "    Parámetros:\n",
    "    row (Series): Fila del DataFrame\n",
    "    backup_counter (int): Cada cuántas filas se debe realizar un backup\n",
    "    \n",
    "    Retorna:\n",
    "    tuple: (latitud, longitud) o None si no se pudo obtener\n",
    "    \"\"\"\n",
    "    # global i  # Contador para backups periódicos\n",
    "    \n",
    "    # Realizar backup periódico\n",
    "    # if i % backup_counter == 0:\n",
    "    #     df_accidents.to_csv(f\"../data/processed/accidents_backup_{i}.csv\", index=False)\n",
    "        \n",
    "    try:\n",
    "        # Si ya existe, mantenerla\n",
    "        if is_valid(row[\"latitud\"]) and is_valid(row[\"longitud\"]):\n",
    "            return row[\"latitud\"], row[\"longitud\"]\n",
    "            \n",
    "        # Si tenemos calle y altura, combinarlas\n",
    "        if is_valid(row[\"calle_y_altura\"]):\n",
    "            #i += 1\n",
    "            return obtener_lat_lon_from_direccion(row[\"calle_y_altura\"])\n",
    "    except Exception as e:\n",
    "        #i += 1\n",
    "        # Manejar errores\n",
    "        print(f\"Error procesando fila {i}: {e}\")\n",
    "        return pd.NA, pd.NA\n",
    "    \n",
    "# Inicializar contador global\n",
    "# i = 0\n",
    "\n",
    "# Aplicar la función a todo el dataset (puede llevar tiempo)\n",
    "# Descomentar para ejecutar:\n",
    "df_accidents[[\"latitud\", \"longitud\"]] = df_accidents.progress_apply( lambda row: pd.Series(obtener_lat_lon(row)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conversión de tipos de datos y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversión de tipos completada\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Manejo de fechas\n",
    "pd.to_datetime(df_accidents[\"fecha\"], errors=\"coerce\")\n",
    "\n",
    "# Manejar horas\n",
    "df_accidents[\"hh\"] = pd.to_numeric(df_accidents[\"hh\"], errors=\"coerce\")\n",
    "df_accidents[\"hh\"] = df_accidents[\"hh\"].astype(\"Int64\")  # Tipo entero que permite NaN\n",
    "df_accidents[\"hh\"] = df_accidents[\"hh\"].replace(0, np.nan) # Se reemplazan ceros por NaN, hora 0 se utiliza para indicar que no se conoce la hora del accidente\n",
    "# Normalizar textos a mayúsculas para consistencia\n",
    "text_columns = [\"calle_y_altura\", \"victima\", \"contraparte\", \"participantes\"]\n",
    "for col in text_columns:\n",
    "    if col in df_accidents.columns:\n",
    "        df_accidents[col] = df_accidents[col].astype(str).str.upper()\n",
    "        # Reemplazar valores no válidos por NaN\n",
    "        df_accidents[col] = df_accidents[col].replace([\"NAN\", \"NONE\", \"NONE NONE\", \"SD\", \"SD-SD\"], np.nan)\n",
    "\n",
    "print(\"Conversión de tipos completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Limpieza específica y correcciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando limpieza de valores específicos...\n",
      "Limpieza específica completada\n"
     ]
    }
   ],
   "source": [
    "# Limpiar valores específicos\n",
    "print(\"Realizando limpieza de valores específicos...\")\n",
    "\n",
    "# Normalizar 'gravedad'\n",
    "df_accidents[\"gravedad\"] = df_accidents[\"gravedad\"].str.upper()\n",
    "df_accidents[\"gravedad\"] = df_accidents[\"gravedad\"].fillna(\"LEVE\")  # Suponiendo que falta = leve\n",
    "\n",
    "# Corregir casos especiales en direcciones\n",
    "# Para autopistas\n",
    "df_accidents[\"calle_y_altura\"] = df_accidents.apply(\n",
    "    lambda row: f\"{row['calle']} Y {row['calle_y_altura']}\" \n",
    "    if str(row['calle']).upper().startswith(\"AUTOPISTA\") and is_valid(row['calle_y_altura']) \n",
    "    else row['calle_y_altura'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Eliminar \"Y SD\" al final de direcciones\n",
    "df_accidents[\"calle_y_altura\"] = df_accidents[\"calle_y_altura\"].str.replace(\" Y SD$\", \"\", regex=True)\n",
    "\n",
    "print(\"Limpieza específica completada\")\n",
    "\n",
    "# Tratamiento de valores \"SD\" a nulos.\n",
    "\n",
    "df_accidents = df_accidents.replace([\"Sd\", \"sd\", \"SD\", np.nan], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Eliminación de registros con datos críticos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros antes de la limpieza: 44012\n",
      "Registros después de la limpieza: 40428\n",
      "Registros eliminados: 3584 (8.14%)\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas críticas que no deben tener valores nulos\n",
    "critical_columns = [\"fecha\", \"calle_y_altura\", \"hh\", \"mm\", \"dd\", \"latitud\", \"longitud\"]\n",
    "\n",
    "# Contar registros antes de la limpieza\n",
    "records_before = len(df_accidents)\n",
    "\n",
    "# Eliminar registros sin datos críticos\n",
    "df_accidents = df_accidents.dropna(subset=critical_columns)\n",
    "\n",
    "# Contar registros después de la limpieza\n",
    "records_after = len(df_accidents)\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "print(f\"Registros antes de la limpieza: {records_before}\")\n",
    "print(f\"Registros después de la limpieza: {records_after}\")\n",
    "print(f\"Registros eliminados: {records_removed} ({records_removed/records_before:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guardar datos procesados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limpio guardado en: ../data/processed/accidents_clean.csv\n",
      "Dimensiones finales: (40428, 19)\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset limpio y procesado\n",
    "output_path = \"../data/processed/accidents_clean.csv\"\n",
    "df_accidents.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset limpio guardado en: {output_path}\")\n",
    "print(f\"Dimensiones finales: {df_accidents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones y próximos pasos\n",
    "\n",
    "Este notebook ha realizado la limpieza y preparación inicial de los datos de siniestros viales en Buenos Aires.  \n",
    "\n",
    "#### Los principales logros son:  \n",
    "\n",
    "    Normalización de direcciones y geocodificación de ubicaciones.  \n",
    "    Corrección de tipos de datos y valores inconsistentes.  \n",
    "    Eliminación de registros incompletos en campos críticos.  \n",
    "\n",
    "#### Próximos pasos:  \n",
    "\n",
    "    Análisis exploratorio inicial de distribución temporal y por hora.  \n",
    "    Visualización espacial preliminar de los accidentes.  \n",
    "    Análisis más profundo por tipo de participantes.  \n",
    "    Identificar patrones espaciotemporales y puntos críticos.  \n",
    "    Correlacionar los accidentes con variables externas (clima, tráfico, etc.).  \n",
    "    Desarrollar modelos predictivos para identificar factores de riesgo.  \n",
    "    Generar recomendaciones para políticas de seguridad vial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
